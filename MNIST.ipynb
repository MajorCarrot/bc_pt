{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb457e0b-ce49-426e-8d18-c93455775d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BinaryLayers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "332c13d6-1c0b-43ce-9707-3249ae6aa586",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "alpha = .15\n",
    "epsilon = 1e-4\n",
    "num_units = 100\n",
    "n_hidden_layers = 1\n",
    "num_epochs = 30\n",
    "dropout_in = 0\n",
    "dropout_hidden = 0\n",
    "\n",
    "deterministic = False\n",
    "H = 1.\n",
    "\n",
    "learning_rate = .001\n",
    "min_learning_rate = 3e-6\n",
    "decay = (learning_rate - min_learning_rate) / num_epochs\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1cf3ba0-4df7-4357-b33b-90bdd417717f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 \n",
      "Epoch training loss 0.1508486959605645\n",
      "Epoch valid loss 0.12568042484613565\n",
      "Epoch 2\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 \n",
      "Epoch training loss 0.09959910818749768\n",
      "Epoch valid loss 0.07486347954433697\n",
      "Epoch 3\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 \n",
      "Epoch training loss 0.059625983166580014\n",
      "Epoch valid loss 0.048246434006171346\n",
      "Epoch 4\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 \n",
      "Epoch training loss 0.042367260560839094\n",
      "Epoch valid loss 0.037320088857832626\n",
      "Epoch 5\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 \n",
      "Epoch training loss 0.03467983772787146\n",
      "Epoch valid loss 0.030838250779570676\n",
      "Epoch 6\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 \n",
      "Epoch training loss 0.030619614383476414\n",
      "Epoch valid loss 0.027720428788318083\n",
      "Epoch 7\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 \n",
      "Epoch training loss 0.027942220837626066\n",
      "Epoch valid loss 0.025357622259224836\n",
      "Epoch 8\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 \n",
      "Epoch training loss 0.02611947645687968\n",
      "Epoch valid loss 0.024219528819697026\n",
      "Epoch 9\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 \n",
      "Epoch training loss 0.024984602722872652\n",
      "Epoch valid loss 0.023517556989995334\n",
      "Epoch 10\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 \n",
      "Epoch training loss 0.023739783375914026\n",
      "Epoch valid loss 0.022617286214461692\n",
      "Epoch 11\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 \n",
      "Epoch training loss 0.02277249640896598\n",
      "Epoch valid loss 0.021591561309133585\n",
      "Epoch 12\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 \n",
      "Epoch training loss 0.02243360252573322\n",
      "Epoch valid loss 0.020407886172716435\n",
      "Epoch 13\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 \n",
      "Epoch training loss 0.021137224295391485\n",
      "Epoch valid loss 0.020315784614724226\n",
      "Epoch 14\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 \n",
      "Epoch training loss 0.021058980047177427\n",
      "Epoch valid loss 0.01939291178654784\n",
      "Epoch 15\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 \n",
      "Epoch training loss 0.020469448108297702\n",
      "Epoch valid loss 0.018867346625297498\n",
      "Epoch 16\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 \n",
      "Epoch training loss 0.01989497270228134\n",
      "Epoch valid loss 0.019171633005428772\n",
      "Epoch 17\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 \n",
      "Epoch training loss 0.01937022122940542\n",
      "Epoch valid loss 0.018318350725353528\n",
      "Epoch 18\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 \n",
      "Epoch training loss 0.018970215028454352\n",
      "Epoch valid loss 0.018187132592384633\n",
      "Epoch 19\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 \n",
      "Epoch training loss 0.01879543608898281\n",
      "Epoch valid loss 0.017731254448732123\n",
      "Epoch 20\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 \n",
      "Epoch training loss 0.01840113669148304\n",
      "Epoch valid loss 0.017906580084504988\n",
      "Epoch 21\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 \n",
      "Epoch training loss 0.018010715331133995\n",
      "Epoch valid loss 0.016842313182468597\n",
      "Epoch 22\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 \n",
      "Epoch training loss 0.017640696697207726\n",
      "Epoch valid loss 0.016739544625847768\n",
      "Epoch 23\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 \n",
      "Epoch training loss 0.017836356908679962\n",
      "Epoch valid loss 0.01678567385683075\n",
      "Epoch 24\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 \n",
      "Epoch training loss 0.01762611117377145\n",
      "Epoch valid loss 0.01635544744726175\n",
      "Epoch 25\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 \n",
      "Epoch training loss 0.017348936090293605\n",
      "Epoch valid loss 0.01680180508022507\n",
      "Epoch 26\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 \n",
      "Epoch training loss 0.016617146373376187\n",
      "Epoch valid loss 0.01613242629294594\n",
      "Epoch 27\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 \n",
      "Epoch training loss 0.016783980124343473\n",
      "Epoch valid loss 0.015455453489453364\n",
      "Epoch 28\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 \n",
      "Epoch training loss 0.01668127553545448\n",
      "Epoch valid loss 0.015983429940369647\n",
      "Epoch 29\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 \n",
      "Epoch training loss 0.016491822770032555\n",
      "Epoch valid loss 0.015675929351112783\n",
      "Epoch 30\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 \n",
      "Epoch training loss 0.016421864612195164\n",
      "Epoch valid loss 0.015742056561299622\n"
     ]
    }
   ],
   "source": [
    "t = transforms.Compose(\n",
    "    [\n",
    "       transforms.ToTensor(),\n",
    "       transforms.Normalize(mean=(0), std=(1))\n",
    "    ]\n",
    ")\n",
    "\n",
    "dl_train = DataLoader(\n",
    "    torchvision.datasets.MNIST(\n",
    "        \"/data/mnist\",\n",
    "        download=True,\n",
    "        train=True,\n",
    "        transform=t,\n",
    "        target_transform=torchvision.transforms.Compose([\n",
    "            lambda x:torch.LongTensor([x]), # or just torch.tensor\n",
    "            lambda x:torch.nn.functional.one_hot(x, 10)\n",
    "        ])\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    "    shuffle=True\n",
    ")\n",
    "dl_valid = DataLoader(\n",
    "    torchvision.datasets.MNIST(\n",
    "        \"/data/mnist\",\n",
    "        download=True,\n",
    "        train=False,\n",
    "        transform=t,\n",
    "        target_transform=torchvision.transforms.Compose([\n",
    "            lambda x:torch.LongTensor([x]), # or just torch.tensor\n",
    "            lambda x:torch.nn.functional.one_hot(x, 10)\n",
    "        ])\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    "    shuffle=True\n",
    ")\n",
    "num_in = 28 * 28\n",
    "\n",
    "layers = []\n",
    "layers.append(torch.nn.Dropout(dropout_in))\n",
    "for i in range(n_hidden_layers):\n",
    "    layers.append(BinaryDense(num_in, num_units, H=H, deterministic=deterministic))\n",
    "#     layers.append(torch.nn.BatchNorm1d(num_units))\n",
    "#     layers.append(torch.nn.Linear(num_in, num_units))\n",
    "    layers.append(torch.nn.Sigmoid())\n",
    "#     layers.append(torch.nn.Dropout(dropout_hidden))\n",
    "    num_in = num_units\n",
    "# layers.append(torch.nn.Linear(num_in, 10))\n",
    "layers.append(BinaryDense(num_in, 10, H=H, deterministic=deterministic))\n",
    "# layers.append(torch.nn.BatchNorm1d(10, eps=epsilon, momentum=alpha))\n",
    "layers.append(torch.nn.Softmax(1))\n",
    "model = torch.nn.Sequential(*layers).to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=30)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=3)\n",
    "lossfunction = torch.nn.MSELoss()\n",
    "\n",
    "losses = [0] * num_epochs\n",
    "val_losses = [0] * num_epochs\n",
    "total_steps = len(dl_train) * num_epochs\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    model.train()\n",
    "    ect = 0\n",
    "    ecv = 0\n",
    "    ett = 0\n",
    "    etv = 0\n",
    "    for i, (input, target) in enumerate(dl_train):\n",
    "        if i % 10 == 0:\n",
    "            print(i, end=\" \")\n",
    "        optimizer.zero_grad()\n",
    "        input = torch.reshape(input, (-1, 28 * 28)).to(device)\n",
    "        target = torch.reshape(target, (-1, 10)).to(device)\n",
    "        output = model(input)\n",
    "        \n",
    "        loss = lossfunction(output, target.float())\n",
    "        losses[epoch] += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for j, (input, target) in enumerate(dl_valid):\n",
    "            input = torch.reshape(input, (-1, 28*28)).to(device)\n",
    "            target = target.reshape((-1, 10)).to(device)\n",
    "            output = model(input)\n",
    "            loss = lossfunction(output, target.float())\n",
    "            val_losses[epoch] += loss.item()\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Epoch training loss\" , losses[epoch] / len(dl_train))\n",
    "    print(\"Epoch valid loss\" , val_losses[epoch] / len(dl_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e81bb4d-b0a6-472b-bf0a-e47f1083f7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8980368589743589\n"
     ]
    }
   ],
   "source": [
    "tot_acc = 0\n",
    "with torch.no_grad():\n",
    "    for j, (input, target) in enumerate(dl_valid):\n",
    "        input = torch.reshape(input, (-1, 28*28)).to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(input)\n",
    "        target = target.argmax(-1).reshape(-1)\n",
    "        output = output.argmax(-1).reshape(-1)\n",
    "        \n",
    "        tot_acc = (tot_acc * j + int(sum(target == output)) / len(target)) / (j + 1)\n",
    "\n",
    "print(\"Validation Accuracy:\", tot_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfce5702-f42f-4c8f-9016-03fbdc7ee49f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
