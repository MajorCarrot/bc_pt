{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcec64bc-3d95-454c-ac63-5ce014cc6bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BinaryLayers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2248b06f-6380-47df-9062-60b011cc0154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Binary Layers\n",
    "H = 1\n",
    "deterministic = True\n",
    "\n",
    "# For Training\n",
    "batch_size = 64\n",
    "num_epochs = 200\n",
    "\n",
    "# For batchnorm\n",
    "epsilon = 1e-4\n",
    "alpha = .1\n",
    "\n",
    "# For the optimizer\n",
    "learning_rate = .3\n",
    "min_learning_rate = 3e-5\n",
    "decay = (learning_rate - min_learning_rate) / num_epochs\n",
    "\n",
    "# Set GPU\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce463854-5458-4776-acc0-b4234922605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn(Module):\n",
    "    def __init__(self):\n",
    "        super(nn, self).__init__()\n",
    "\n",
    "        self.c3_128_1 = BinaryConv2D(3, 128, 3, padding=1, H=H, deterministic=deterministic)\n",
    "        self.bn_1 = torch.nn.BatchNorm2d(128, eps=epsilon, momentum=alpha)\n",
    "\n",
    "        self.c3_128_2 = BinaryConv2D(128, 128, 3, padding=1, H=H, deterministic=deterministic)\n",
    "        self.mp2_2 = torch.nn.MaxPool2d(2)\n",
    "        self.bn_2 = torch.nn.BatchNorm2d(128, eps=epsilon, momentum=alpha)\n",
    "\n",
    "        self.c3_256_3 = BinaryConv2D(128, 256, 3, padding=1, H=H, deterministic=deterministic)\n",
    "        self.bn_3 = torch.nn.BatchNorm2d(256, eps=epsilon, momentum=alpha)\n",
    "\n",
    "        self.c3_256_4 = BinaryConv2D(256, 256, 3, padding=1, H=H, deterministic=deterministic)\n",
    "        self.mp2_4 = torch.nn.MaxPool2d(2)\n",
    "        self.bn_4 = torch.nn.BatchNorm2d(256, eps=epsilon, momentum=alpha)\n",
    "\n",
    "        self.c3_512_5 = BinaryConv2D(256, 512, 3, padding=1, H=H, deterministic=deterministic)\n",
    "        self.bn_5 = torch.nn.BatchNorm2d(512, eps=epsilon, momentum=alpha)\n",
    "\n",
    "        self.c3_512_6 = BinaryConv2D(512, 512, 3, padding=1, H=H, deterministic=deterministic)\n",
    "        self.mp2_6 = torch.nn.MaxPool2d(2)\n",
    "        self.bn_6 = torch.nn.BatchNorm2d(512, eps=epsilon, momentum=alpha)\n",
    "\n",
    "        self.d_1024_7 = BinaryDense(2048 * 4, 1024, H=H, deterministic=deterministic)\n",
    "        self.bn_7 = torch.nn.BatchNorm1d(1024, eps=epsilon, momentum=alpha)\n",
    "        \n",
    "        self.d_1024_8 = BinaryDense(1024, 1024, H=H, deterministic=deterministic)\n",
    "        self.bn_8 = torch.nn.BatchNorm1d(1024, eps=epsilon, momentum=alpha)\n",
    "\n",
    "        self.d_10_9 = BinaryDense(1024, 10, H=H, deterministic=deterministic)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.c3_128_1(input)\n",
    "        x = torch.nn.ReLU()(self.bn_1(x))\n",
    "#         print(x.shape)\n",
    "        x = self.c3_128_2(x)\n",
    "        x = self.mp2_2(x)\n",
    "        x = torch.nn.ReLU()(self.bn_2(x))\n",
    "#         print(x.shape)\n",
    "\n",
    "        x = self.c3_256_3(x)\n",
    "        x = torch.nn.ReLU()(self.bn_3(x))\n",
    "#         print(x.shape)\n",
    "        x = self.c3_256_4(x)\n",
    "        x = self.mp2_4(x)\n",
    "        x = torch.nn.ReLU()(self.bn_4(x))\n",
    "#         print(x.shape)\n",
    "\n",
    "        x = self.c3_512_5(x)\n",
    "        x = torch.nn.ReLU()(self.bn_5(x))\n",
    "#         print(x.shape)\n",
    "        x = self.c3_512_6(x)\n",
    "        x = self.mp2_6(x)\n",
    "        x = torch.nn.ReLU()(self.bn_6(x))\n",
    "#         print(x.shape)\n",
    "\n",
    "        x = torch.nn.Flatten()(x)\n",
    "        \n",
    "        x = self.d_1024_7(x)\n",
    "        x = torch.nn.ReLU()(self.bn_7(x))\n",
    "#         print(x.shape)\n",
    "\n",
    "        x = self.d_1024_8(x)\n",
    "        x = torch.nn.ReLU()(self.bn_8(x))\n",
    "#         print(x.shape)\n",
    "\n",
    "        x = self.d_10_9(x)\n",
    "        x = torch.nn.Softmax(-1)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "361563b3-61f6-45f7-9c05-f67d0a8dd8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "225376c1-d7d5-40ca-bef4-e649a4f0f7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nn(\n",
       "  (c3_128_1): BinaryConv2D(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn_1): BatchNorm2d(128, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (c3_128_2): BinaryConv2D(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (mp2_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (bn_2): BatchNorm2d(128, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (c3_256_3): BinaryConv2D(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn_3): BatchNorm2d(256, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (c3_256_4): BinaryConv2D(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (mp2_4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (bn_4): BatchNorm2d(256, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (c3_512_5): BinaryConv2D(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn_5): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (c3_512_6): BinaryConv2D(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (mp2_6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (bn_6): BatchNorm2d(512, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (d_1024_7): BinaryDense(in_features=8192, out_features=1024, bias=False)\n",
       "  (bn_7): BatchNorm1d(1024, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (d_1024_8): BinaryDense(in_features=1024, out_features=1024, bias=False)\n",
       "  (bn_8): BatchNorm1d(1024, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (d_10_9): BinaryDense(in_features=1024, out_features=10, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12819563-1100-4938-9e51-1f7ba854003e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "t = transforms.Compose(\n",
    "    [\n",
    "       transforms.ToTensor(),\n",
    "       transforms.Normalize(mean=(0), std=(1))\n",
    "    ]\n",
    ")\n",
    "\n",
    "dl_train = DataLoader(\n",
    "    torchvision.datasets.CIFAR10(\n",
    "        \"/data/cifar\",\n",
    "        download=True,\n",
    "        train=True,\n",
    "        transform=t,\n",
    "        target_transform=torchvision.transforms.Compose([\n",
    "            lambda x:torch.LongTensor([x]), # or just torch.tensor\n",
    "            lambda x:torch.nn.functional.one_hot(x, 10)\n",
    "        ])\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    "    shuffle=True\n",
    ")\n",
    "dl_test = DataLoader(\n",
    "    torchvision.datasets.CIFAR10(\n",
    "        \"/data/cifar\",\n",
    "        download=True,\n",
    "        train=False,\n",
    "        transform=t,\n",
    "        target_transform=torchvision.transforms.Compose([\n",
    "            lambda x:torch.LongTensor([x]), # or just torch.tensor\n",
    "            lambda x:torch.nn.functional.one_hot(x, 10)\n",
    "        ])\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d810614-fc5c-4902-9c03-0564d613f47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = SquareHingeLoss\n",
    "loss = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24cf9650-8320-43da-b83e-b8e9d972f783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs, dl_train, dl_valid, optimizer, lossfunction):\n",
    "    losses = [0] * num_epochs\n",
    "    val_losses = [0] * num_epochs\n",
    "    accuracy = [0] * num_epochs\n",
    "    \n",
    "    total_steps = len(dl_train) * num_epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}\")\n",
    "        # Start training\n",
    "        model.train()\n",
    "        for i, (input, target) in enumerate(dl_train):\n",
    "            if i % 10 == 0:\n",
    "                print(i, end=\" \")\n",
    "            optimizer.zero_grad()\n",
    "#             input = torch.reshape(input, (-1, 32 * 32)).to(device)\n",
    "            input = input.to(device)\n",
    "            target = torch.reshape(target, (-1, 10)).to(device)\n",
    "            output = model(input)\n",
    "\n",
    "            loss = lossfunction(output, target.float())\n",
    "            losses[epoch] += loss.item()\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "        model.eval()\n",
    "        tot_acc = 0\n",
    "        with torch.no_grad():\n",
    "            for j, (input, target) in enumerate(dl_valid):\n",
    "#                 input = torch.reshape(input, (-1, 32 * 32)).to(device)\n",
    "                input = input.to(device)\n",
    "                target = target.reshape((-1, 10)).to(device)\n",
    "                output = model(input)\n",
    "                loss = lossfunction(output, target.float())\n",
    "                val_losses[epoch] += loss.item()\n",
    "                tot_acc = (tot_acc * j + int(sum(torch.argmax(target, -1) == torch.argmax(output, -1))) / len(target)) / (j + 1)\n",
    "        accuracy[epoch] = tot_acc\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Epoch training loss\" , losses[epoch] / len(dl_train))\n",
    "        print(\"Epoch valid loss\" , val_losses[epoch] / len(dl_valid))\n",
    "        print(\"Validation Accuracy:\", tot_acc)\n",
    "    return losses, val_losses, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed7ad05e-7054-4379-869c-ae9cdf6429e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.1509904336284431\n",
      "Epoch valid loss 0.1391333234615815\n",
      "Validation Accuracy: 0.2864583333333335\n",
      "Epoch 2\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.1333512716023931\n",
      "Epoch valid loss 0.13089958530588028\n",
      "Validation Accuracy: 0.3310296474358976\n",
      "Epoch 3\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.12486946732568985\n",
      "Epoch valid loss 0.11984794730177292\n",
      "Validation Accuracy: 0.3852163461538461\n",
      "Epoch 4\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.1179105054954408\n",
      "Epoch valid loss 0.11048950102084722\n",
      "Validation Accuracy: 0.43279246794871795\n",
      "Epoch 5\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.11212415324473961\n",
      "Epoch valid loss 0.10404857921485718\n",
      "Validation Accuracy: 0.46564503205128205\n",
      "Epoch 6\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.10797838050104133\n",
      "Epoch valid loss 0.10782388411462307\n",
      "Validation Accuracy: 0.44771634615384615\n",
      "Epoch 7\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.10261448255230919\n",
      "Epoch valid loss 0.10090264625465259\n",
      "Validation Accuracy: 0.4827724358974359\n",
      "Epoch 8\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.09964284349457098\n",
      "Epoch valid loss 0.10380798391997814\n",
      "Validation Accuracy: 0.4680488782051282\n",
      "Epoch 9\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.0953216315431952\n",
      "Epoch valid loss 0.09436129496838802\n",
      "Validation Accuracy: 0.5154246794871798\n",
      "Epoch 10\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.0916363684825418\n",
      "Epoch valid loss 0.09270799711633188\n",
      "Validation Accuracy: 0.5247395833333329\n",
      "Epoch 11\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.08900736973510051\n",
      "Epoch valid loss 0.08887614123523235\n",
      "Validation Accuracy: 0.5440705128205128\n",
      "Epoch 12\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.08515465551946746\n",
      "Epoch valid loss 0.08319427147030066\n",
      "Validation Accuracy: 0.5726161858974359\n",
      "Epoch 13\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.08012744237724859\n",
      "Epoch valid loss 0.08397768194285724\n",
      "Validation Accuracy: 0.5665064102564104\n",
      "Epoch 14\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.07812941194572766\n",
      "Epoch valid loss 0.07955207986136277\n",
      "Validation Accuracy: 0.5891426282051284\n",
      "Epoch 15\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.07444340834410793\n",
      "Epoch valid loss 0.07463558498196877\n",
      "Validation Accuracy: 0.6145833333333335\n",
      "Epoch 16\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.07197701963435062\n",
      "Epoch valid loss 0.07451009857826509\n",
      "Validation Accuracy: 0.6145833333333331\n",
      "Epoch 17\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.0698996432287745\n",
      "Epoch valid loss 0.07412182023892036\n",
      "Validation Accuracy: 0.6174879807692307\n",
      "Epoch 18\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.06645820634236867\n",
      "Epoch valid loss 0.07220458707366234\n",
      "Validation Accuracy: 0.6282051282051284\n",
      "Epoch 19\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.06492678820132904\n",
      "Epoch valid loss 0.0677153140019912\n",
      "Validation Accuracy: 0.6495392628205124\n",
      "Epoch 20\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.06257808841431034\n",
      "Epoch valid loss 0.06720387429381028\n",
      "Validation Accuracy: 0.6534455128205128\n",
      "Epoch 21\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.05995395828262639\n",
      "Epoch valid loss 0.06648026468853156\n",
      "Validation Accuracy: 0.655849358974359\n",
      "Epoch 22\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.05830353373487536\n",
      "Epoch valid loss 0.06485961709553614\n",
      "Validation Accuracy: 0.6650641025641026\n",
      "Epoch 23\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.056799439095657094\n",
      "Epoch valid loss 0.0629511119869466\n",
      "Validation Accuracy: 0.6750801282051281\n",
      "Epoch 24\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.055045487485568444\n",
      "Epoch valid loss 0.06222404052431767\n",
      "Validation Accuracy: 0.6786858974358975\n",
      "Epoch 25\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.053239485656749574\n",
      "Epoch valid loss 0.06142106030184107\n",
      "Validation Accuracy: 0.6831931089743589\n",
      "Epoch 26\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.051438720114576834\n",
      "Epoch valid loss 0.05756369194923303\n",
      "Validation Accuracy: 0.7014222756410254\n",
      "Epoch 27\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.049188026018373944\n",
      "Epoch valid loss 0.05934272878445112\n",
      "Validation Accuracy: 0.6937099358974358\n",
      "Epoch 28\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.04786603836278299\n",
      "Epoch valid loss 0.056939809141346276\n",
      "Validation Accuracy: 0.7054286858974359\n",
      "Epoch 29\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.047031778618025565\n",
      "Epoch valid loss 0.05661685943890076\n",
      "Validation Accuracy: 0.7068309294871795\n",
      "Epoch 30\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.04503301464536355\n",
      "Epoch valid loss 0.05394551397946019\n",
      "Validation Accuracy: 0.7205528846153845\n",
      "Epoch 31\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.04440531857602689\n",
      "Epoch valid loss 0.05311108548910572\n",
      "Validation Accuracy: 0.7249599358974358\n",
      "Epoch 32\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.042539946441556516\n",
      "Epoch valid loss 0.054612862292486124\n",
      "Validation Accuracy: 0.7180488782051285\n",
      "Epoch 33\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.041810790738675184\n",
      "Epoch valid loss 0.05226028603143417\n",
      "Validation Accuracy: 0.7296674679487178\n",
      "Epoch 34\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.04019238028517911\n",
      "Epoch valid loss 0.051206945370023065\n",
      "Validation Accuracy: 0.7345753205128204\n",
      "Epoch 35\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.03889431717844916\n",
      "Epoch valid loss 0.05012008897625865\n",
      "Validation Accuracy: 0.7407852564102568\n",
      "Epoch 36\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.03849626727349741\n",
      "Epoch valid loss 0.049774287172999136\n",
      "Validation Accuracy: 0.7413862179487182\n",
      "Epoch 37\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.03735453594313808\n",
      "Epoch valid loss 0.049155443285902344\n",
      "Validation Accuracy: 0.7458934294871793\n",
      "Epoch 38\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.03703854719675343\n",
      "Epoch valid loss 0.04675881393874685\n",
      "Validation Accuracy: 0.7572115384615383\n",
      "Epoch 39\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.03513910618184967\n",
      "Epoch valid loss 0.04770501891676432\n",
      "Validation Accuracy: 0.7522035256410258\n",
      "Epoch 40\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.034498600251066394\n",
      "Epoch valid loss 0.04639562042668844\n",
      "Validation Accuracy: 0.7585136217948718\n",
      "Epoch 41\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.033375125360781285\n",
      "Epoch valid loss 0.04777500112182819\n",
      "Validation Accuracy: 0.7510016025641025\n",
      "Epoch 42\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.03291539944977846\n",
      "Epoch valid loss 0.04592915748556455\n",
      "Validation Accuracy: 0.7609174679487182\n",
      "Epoch 43\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.03183692702832757\n",
      "Epoch valid loss 0.046319586045753494\n",
      "Validation Accuracy: 0.7580128205128206\n",
      "Epoch 44\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.03049537988925751\n",
      "Epoch valid loss 0.04469549156223925\n",
      "Validation Accuracy: 0.7684294871794872\n",
      "Epoch 45\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.030617768719444164\n",
      "Epoch valid loss 0.043937627596255295\n",
      "Validation Accuracy: 0.7722355769230766\n",
      "Epoch 46\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.029611918027549994\n",
      "Epoch valid loss 0.045669541049462095\n",
      "Validation Accuracy: 0.7617187500000001\n",
      "Epoch 47\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.02919734638332176\n",
      "Epoch valid loss 0.04395591009121675\n",
      "Validation Accuracy: 0.7705328525641024\n",
      "Epoch 48\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.028196989577120496\n",
      "Epoch valid loss 0.04578313919214102\n",
      "Validation Accuracy: 0.7618189102564105\n",
      "Epoch 49\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.02767711579257076\n",
      "Epoch valid loss 0.043729407366556235\n",
      "Validation Accuracy: 0.7720352564102563\n",
      "Epoch 50\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.02698206381750664\n",
      "Epoch valid loss 0.043980894407305196\n",
      "Validation Accuracy: 0.7714342948717948\n",
      "Epoch 51\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.026498841291720407\n",
      "Epoch valid loss 0.044214073329781875\n",
      "Validation Accuracy: 0.7701322115384616\n",
      "Epoch 52\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.026298887567149258\n",
      "Epoch valid loss 0.042980012585385106\n",
      "Validation Accuracy: 0.7760416666666666\n",
      "Epoch 53\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.025742007362824434\n",
      "Epoch valid loss 0.04378448136580678\n",
      "Validation Accuracy: 0.7708333333333336\n",
      "Epoch 54\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.024876297000502253\n",
      "Epoch valid loss 0.0424278268041328\n",
      "Validation Accuracy: 0.7800480769230769\n",
      "Epoch 55\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.024577515047739966\n",
      "Epoch valid loss 0.04353309806006459\n",
      "Validation Accuracy: 0.7728365384615384\n",
      "Epoch 56\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.02373862950340334\n",
      "Epoch valid loss 0.04259345164665809\n",
      "Validation Accuracy: 0.7785456730769231\n",
      "Epoch 57\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.022880356816667943\n",
      "Epoch valid loss 0.042099374400165215\n",
      "Validation Accuracy: 0.7813501602564104\n",
      "Epoch 58\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.022715919952537935\n",
      "Epoch valid loss 0.04187344523887031\n",
      "Validation Accuracy: 0.7823517628205129\n",
      "Epoch 59\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.021758843948182182\n",
      "Epoch valid loss 0.040606307522513166\n",
      "Validation Accuracy: 0.7884615384615384\n",
      "Epoch 60\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.021128617286104972\n",
      "Epoch valid loss 0.04164057365881327\n",
      "Validation Accuracy: 0.7833533653846155\n",
      "Epoch 61\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.02056737633710596\n",
      "Epoch valid loss 0.04174737568992453\n",
      "Validation Accuracy: 0.7817508012820513\n",
      "Epoch 62\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.020432462291075357\n",
      "Epoch valid loss 0.041934672420701154\n",
      "Validation Accuracy: 0.7815504807692308\n",
      "Epoch 63\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.019642584308178168\n",
      "Epoch valid loss 0.0415191528482888\n",
      "Validation Accuracy: 0.7832532051282052\n",
      "Epoch 64\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.019333558702540245\n",
      "Epoch valid loss 0.041211469910847835\n",
      "Validation Accuracy: 0.7845552884615382\n",
      "Epoch 65\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.01902132839364016\n",
      "Epoch valid loss 0.040509726613377914\n",
      "Validation Accuracy: 0.7890625\n",
      "Epoch 66\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.018488636864558303\n",
      "Epoch valid loss 0.040818478112132885\n",
      "Validation Accuracy: 0.7867588141025644\n",
      "Epoch 67\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.017952392068571\n",
      "Epoch valid loss 0.04087238100906595\n",
      "Validation Accuracy: 0.7875600961538465\n",
      "Epoch 68\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.018047354036522373\n",
      "Epoch valid loss 0.04144963352248455\n",
      "Validation Accuracy: 0.7843549679487182\n",
      "Epoch 69\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.017440764268517532\n",
      "Epoch valid loss 0.040537663377248324\n",
      "Validation Accuracy: 0.7877604166666666\n",
      "Epoch 70\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.016658522032120437\n",
      "Epoch valid loss 0.039368404779965296\n",
      "Validation Accuracy: 0.7941706730769232\n",
      "Epoch 71\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.016880934156900694\n",
      "Epoch valid loss 0.0405133103665251\n",
      "Validation Accuracy: 0.7877604166666671\n",
      "Epoch 72\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.016750362758057677\n",
      "Epoch valid loss 0.0408179885468995\n",
      "Validation Accuracy: 0.7871594551282052\n",
      "Epoch 73\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.01626473123624991\n",
      "Epoch valid loss 0.039477956684258506\n",
      "Validation Accuracy: 0.7943709935897436\n",
      "Epoch 74\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.015569147804605811\n",
      "Epoch valid loss 0.03888571968612572\n",
      "Validation Accuracy: 0.7966746794871796\n",
      "Epoch 75\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.015876974036548554\n",
      "Epoch valid loss 0.03996021039067553\n",
      "Validation Accuracy: 0.7918669871794871\n",
      "Epoch 76\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.015253726504165526\n",
      "Epoch valid loss 0.0398881062387656\n",
      "Validation Accuracy: 0.7918669871794872\n",
      "Epoch 77\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.014448823712972052\n",
      "Epoch valid loss 0.03962397404635946\n",
      "Validation Accuracy: 0.7936698717948719\n",
      "Epoch 78\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.0143583677052904\n",
      "Epoch valid loss 0.03924072157734862\n",
      "Validation Accuracy: 0.7944711538461539\n",
      "Epoch 79\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.014176210669635708\n",
      "Epoch valid loss 0.039668756656539746\n",
      "Validation Accuracy: 0.79306891025641\n",
      "Epoch 80\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.013977328264851901\n",
      "Epoch valid loss 0.0392631633947484\n",
      "Validation Accuracy: 0.7940705128205128\n",
      "Epoch 81\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.013731040069429122\n",
      "Epoch valid loss 0.038920584481018476\n",
      "Validation Accuracy: 0.7975761217948718\n",
      "Epoch 82\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.013194838259929885\n",
      "Epoch valid loss 0.039290628694475464\n",
      "Validation Accuracy: 0.794571314102564\n",
      "Epoch 83\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.013306053231399554\n",
      "Epoch valid loss 0.039760585384777725\n",
      "Validation Accuracy: 0.7918669871794873\n",
      "Epoch 84\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.012775749655228855\n",
      "Epoch valid loss 0.03942568650922905\n",
      "Validation Accuracy: 0.7942708333333334\n",
      "Epoch 85\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.012500725559301425\n",
      "Epoch valid loss 0.038509080783487894\n",
      "Validation Accuracy: 0.7990785256410257\n",
      "Epoch 86\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.012124931037175586\n",
      "Epoch valid loss 0.038160869326346956\n",
      "Validation Accuracy: 0.7999799679487178\n",
      "Epoch 87\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.012038381576859018\n",
      "Epoch valid loss 0.03842713406834847\n",
      "Validation Accuracy: 0.8002804487179486\n",
      "Epoch 88\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.01203930073706236\n",
      "Epoch valid loss 0.039378631967478074\n",
      "Validation Accuracy: 0.7938701923076922\n",
      "Epoch 89\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.011663729909688149\n",
      "Epoch valid loss 0.039829151334766395\n",
      "Validation Accuracy: 0.7925681089743589\n",
      "Epoch 90\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.011290739377386621\n",
      "Epoch valid loss 0.038626466788208254\n",
      "Validation Accuracy: 0.7971754807692307\n",
      "Epoch 91\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.011192734097059358\n",
      "Epoch valid loss 0.038455369183793664\n",
      "Validation Accuracy: 0.7990785256410257\n",
      "Epoch 92\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.011304083978933706\n",
      "Epoch valid loss 0.03858653478658734\n",
      "Validation Accuracy: 0.7993790064102564\n",
      "Epoch 93\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.010824140372399033\n",
      "Epoch valid loss 0.03810163037493252\n",
      "Validation Accuracy: 0.8013822115384617\n",
      "Epoch 94\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.010456413335542586\n",
      "Epoch valid loss 0.0377959215010588\n",
      "Validation Accuracy: 0.8025841346153848\n",
      "Epoch 95\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.010305673021181572\n",
      "Epoch valid loss 0.037577068606296025\n",
      "Validation Accuracy: 0.8043870192307693\n",
      "Epoch 96\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.010315159039620436\n",
      "Epoch valid loss 0.03777523778784925\n",
      "Validation Accuracy: 0.8024839743589741\n",
      "Epoch 97\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.010277679817114676\n",
      "Epoch valid loss 0.037979415904443994\n",
      "Validation Accuracy: 0.8024839743589741\n",
      "Epoch 98\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.010245146989729623\n",
      "Epoch valid loss 0.03781315198359199\n",
      "Validation Accuracy: 0.802383814102564\n",
      "Epoch 99\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.009757926142468757\n",
      "Epoch valid loss 0.03755482686205934\n",
      "Validation Accuracy: 0.8034855769230765\n",
      "Epoch 100\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.009758055428096326\n",
      "Epoch valid loss 0.03918298363136367\n",
      "Validation Accuracy: 0.7949719551282051\n",
      "Epoch 101\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.009794348346234912\n",
      "Epoch valid loss 0.038464247225186765\n",
      "Validation Accuracy: 0.8001802884615384\n",
      "Epoch 102\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.00954262542835259\n",
      "Epoch valid loss 0.03824963938229932\n",
      "Validation Accuracy: 0.8003806089743589\n",
      "Epoch 103\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.009544482588283441\n",
      "Epoch valid loss 0.037750963133592635\n",
      "Validation Accuracy: 0.8036858974358974\n",
      "Epoch 104\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.00932814104011176\n",
      "Epoch valid loss 0.03837173286443337\n",
      "Validation Accuracy: 0.7998798076923077\n",
      "Epoch 105\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.00931732559589449\n",
      "Epoch valid loss 0.03817186011479069\n",
      "Validation Accuracy: 0.8010817307692308\n",
      "Epoch 106\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.009100888132404302\n",
      "Epoch valid loss 0.038444338676830135\n",
      "Validation Accuracy: 0.7988782051282052\n",
      "Epoch 107\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.008989839492113894\n",
      "Epoch valid loss 0.0373367090912488\n",
      "Validation Accuracy: 0.8043870192307693\n",
      "Epoch 108\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.008617450614350817\n",
      "Epoch valid loss 0.03793048677154076\n",
      "Validation Accuracy: 0.8010817307692306\n",
      "Epoch 109\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.00856184634524431\n",
      "Epoch valid loss 0.03776996388720969\n",
      "Validation Accuracy: 0.8026842948717949\n",
      "Epoch 110\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.008746168412664656\n",
      "Epoch valid loss 0.03668359488559266\n",
      "Validation Accuracy: 0.8075921474358976\n",
      "Epoch 111\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.008564944768200317\n",
      "Epoch valid loss 0.03754126133683782\n",
      "Validation Accuracy: 0.8031850961538464\n",
      "Epoch 112\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.00815458059065534\n",
      "Epoch valid loss 0.037871700938408956\n",
      "Validation Accuracy: 0.8019831730769231\n",
      "Epoch 113\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.00839716499482073\n",
      "Epoch valid loss 0.03730551178495471\n",
      "Validation Accuracy: 0.8039863782051281\n",
      "Epoch 114\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.008169921638032044\n",
      "Epoch valid loss 0.037139695681010686\n",
      "Validation Accuracy: 0.806189903846154\n",
      "Epoch 115\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.008184248038393047\n",
      "Epoch valid loss 0.037184623249161705\n",
      "Validation Accuracy: 0.8052884615384617\n",
      "Epoch 116\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.008064083632619097\n",
      "Epoch valid loss 0.03851183120591136\n",
      "Validation Accuracy: 0.799178685897436\n",
      "Epoch 117\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.00799816530344933\n",
      "Epoch valid loss 0.03765120113698336\n",
      "Validation Accuracy: 0.8029847756410257\n",
      "Epoch 118\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.00786940243614327\n",
      "Epoch valid loss 0.03750844278301184\n",
      "Validation Accuracy: 0.8031850961538458\n",
      "Epoch 119\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.007850960342523309\n",
      "Epoch valid loss 0.038064177470425\n",
      "Validation Accuracy: 0.8002804487179487\n",
      "Epoch 120\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.007807026820244451\n",
      "Epoch valid loss 0.03755632984953431\n",
      "Validation Accuracy: 0.8041866987179486\n",
      "Epoch 121\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.007704481084612909\n",
      "Epoch valid loss 0.038142402698166475\n",
      "Validation Accuracy: 0.8012820512820513\n",
      "Epoch 122\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.007594241834345001\n",
      "Epoch valid loss 0.03760566420327777\n",
      "Validation Accuracy: 0.8029847756410255\n",
      "Epoch 123\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.007368068421232029\n",
      "Epoch valid loss 0.03644823397581394\n",
      "Validation Accuracy: 0.8102964743589742\n",
      "Epoch 124\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.007553882286918527\n",
      "Epoch valid loss 0.036642905933639176\n",
      "Validation Accuracy: 0.808193108974359\n",
      "Epoch 125\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.00756577655012885\n",
      "Epoch valid loss 0.036867050925651804\n",
      "Validation Accuracy: 0.8067908653846155\n",
      "Epoch 126\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.0074512750642800754\n",
      "Epoch valid loss 0.03724866934144535\n",
      "Validation Accuracy: 0.8056891025641025\n",
      "Epoch 127\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.007431705694575351\n",
      "Epoch valid loss 0.03697395017848183\n",
      "Validation Accuracy: 0.8067908653846152\n",
      "Epoch 128\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.007454820286334893\n",
      "Epoch valid loss 0.03738602573195329\n",
      "Validation Accuracy: 0.8036858974358976\n",
      "Epoch 129\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.007211375606936656\n",
      "Epoch valid loss 0.03645049871948476\n",
      "Validation Accuracy: 0.8099959935897435\n",
      "Epoch 130\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.0071058419021877125\n",
      "Epoch valid loss 0.037249536731113225\n",
      "Validation Accuracy: 0.804987980769231\n",
      "Epoch 131\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.007255310649588246\n",
      "Epoch valid loss 0.03740226172913726\n",
      "Validation Accuracy: 0.8039863782051285\n",
      "Epoch 132\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.0069878511250643\n",
      "Epoch valid loss 0.03708318637636227\n",
      "Validation Accuracy: 0.8050881410256412\n",
      "Epoch 133\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.0068996987982457665\n",
      "Epoch valid loss 0.03668834196212582\n",
      "Validation Accuracy: 0.8078926282051281\n",
      "Epoch 134\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.006932487258796601\n",
      "Epoch valid loss 0.03658709085235993\n",
      "Validation Accuracy: 0.8089943910256411\n",
      "Epoch 135\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.006928539648458862\n",
      "Epoch valid loss 0.03711722455679988\n",
      "Validation Accuracy: 0.8055889423076923\n",
      "Epoch 136\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.006878574484570905\n",
      "Epoch valid loss 0.03704572398549853\n",
      "Validation Accuracy: 0.8069911858974362\n",
      "Epoch 137\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.006735717935441893\n",
      "Epoch valid loss 0.03621497663120047\n",
      "Validation Accuracy: 0.8096955128205128\n",
      "Epoch 138\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.0067589074550572\n",
      "Epoch valid loss 0.03608384579181289\n",
      "Validation Accuracy: 0.8105969551282051\n",
      "Epoch 139\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.006702740086936324\n",
      "Epoch valid loss 0.036140669184999585\n",
      "Validation Accuracy: 0.8103966346153847\n",
      "Epoch 140\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.00651605628929146\n",
      "Epoch valid loss 0.03598763052230844\n",
      "Validation Accuracy: 0.8115985576923078\n",
      "Epoch 141\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.0065195365081028925\n",
      "Epoch valid loss 0.03710542127299003\n",
      "Validation Accuracy: 0.8054887820512822\n",
      "Epoch 142\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.006529420679371288\n",
      "Epoch valid loss 0.03629375906446232\n",
      "Validation Accuracy: 0.8105969551282053\n",
      "Epoch 143\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.006428836023950918\n",
      "Epoch valid loss 0.037153456575022295\n",
      "Validation Accuracy: 0.80478766025641\n",
      "Epoch 144\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.00650230666957578\n",
      "Epoch valid loss 0.03679539018477767\n",
      "Validation Accuracy: 0.8078926282051285\n",
      "Epoch 145\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.006455209547563961\n",
      "Epoch valid loss 0.03752394751287424\n",
      "Validation Accuracy: 0.8048878205128207\n",
      "Epoch 146\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.006514643452496863\n",
      "Epoch valid loss 0.03711173763403144\n",
      "Validation Accuracy: 0.8055889423076923\n",
      "Epoch 147\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.006352154595502652\n",
      "Epoch valid loss 0.036550845043399394\n",
      "Validation Accuracy: 0.8083934294871796\n",
      "Epoch 148\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.00643736722514426\n",
      "Epoch valid loss 0.036863118720551334\n",
      "Validation Accuracy: 0.8070913461538459\n",
      "Epoch 149\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.00638712889630851\n",
      "Epoch valid loss 0.03710441718785427\n",
      "Validation Accuracy: 0.8060897435897436\n",
      "Epoch 150\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.006332451011659406\n",
      "Epoch valid loss 0.036609453916287005\n",
      "Validation Accuracy: 0.8071915064102564\n",
      "Epoch 151\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.0063424238526429986\n",
      "Epoch valid loss 0.036881703399838164\n",
      "Validation Accuracy: 0.8074919871794874\n",
      "Epoch 152\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.0061833554447225415\n",
      "Epoch valid loss 0.03602241775474678\n",
      "Validation Accuracy: 0.8113982371794872\n",
      "Epoch 153\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.006160912671349505\n",
      "Epoch valid loss 0.03617168974895508\n",
      "Validation Accuracy: 0.8110977564102563\n",
      "Epoch 154\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.006137085022939912\n",
      "Epoch valid loss 0.0361104424421986\n",
      "Validation Accuracy: 0.8113982371794872\n",
      "Epoch 155\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.0059967191835226955\n",
      "Epoch valid loss 0.037394370507592194\n",
      "Validation Accuracy: 0.8041866987179492\n",
      "Epoch 156\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.006096530208700222\n",
      "Epoch valid loss 0.03669025711954022\n",
      "Validation Accuracy: 0.8083934294871795\n",
      "Epoch 157\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.00619852341057839\n",
      "Epoch valid loss 0.03619774974858723\n",
      "Validation Accuracy: 0.8108974358974358\n",
      "Epoch 158\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.006002890579702938\n",
      "Epoch valid loss 0.036356626239676886\n",
      "Validation Accuracy: 0.809795673076923\n",
      "Epoch 159\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005937962374186403\n",
      "Epoch valid loss 0.03679659679675332\n",
      "Validation Accuracy: 0.8077924679487177\n",
      "Epoch 160\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005881804274440127\n",
      "Epoch valid loss 0.0361074253391379\n",
      "Validation Accuracy: 0.8107972756410254\n",
      "Epoch 161\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005892568154987296\n",
      "Epoch valid loss 0.03661536787732098\n",
      "Validation Accuracy: 0.8092948717948719\n",
      "Epoch 162\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005938274997351854\n",
      "Epoch valid loss 0.036287049356943525\n",
      "Validation Accuracy: 0.8097956730769234\n",
      "Epoch 163\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005862797625262003\n",
      "Epoch valid loss 0.0363970210787673\n",
      "Validation Accuracy: 0.8095953525641024\n",
      "Epoch 164\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005812465027415429\n",
      "Epoch valid loss 0.03605593958248695\n",
      "Validation Accuracy: 0.8111979166666666\n",
      "Epoch 165\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005833410167379187\n",
      "Epoch valid loss 0.0363681063008232\n",
      "Validation Accuracy: 0.8107972756410255\n",
      "Epoch 166\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005673864709103294\n",
      "Epoch valid loss 0.035248716678231574\n",
      "Validation Accuracy: 0.8161057692307693\n",
      "Epoch 167\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005700172678758161\n",
      "Epoch valid loss 0.03587804780079004\n",
      "Validation Accuracy: 0.8119991987179487\n",
      "Epoch 168\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005723729077404061\n",
      "Epoch valid loss 0.0351142664738477\n",
      "Validation Accuracy: 0.8155048076923076\n",
      "Epoch 169\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005755702780219445\n",
      "Epoch valid loss 0.03563577052540122\n",
      "Validation Accuracy: 0.8137019230769231\n",
      "Epoch 170\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005611815909345295\n",
      "Epoch valid loss 0.03621952356293033\n",
      "Validation Accuracy: 0.8112980769230768\n",
      "Epoch 171\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.0056717031765341555\n",
      "Epoch valid loss 0.03676650247488839\n",
      "Validation Accuracy: 0.8076923076923078\n",
      "Epoch 172\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005674224997100925\n",
      "Epoch valid loss 0.03578675204577545\n",
      "Validation Accuracy: 0.8135016025641024\n",
      "Epoch 173\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005757262148505495\n",
      "Epoch valid loss 0.03622888309809451\n",
      "Validation Accuracy: 0.8105969551282052\n",
      "Epoch 174\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005542200126607051\n",
      "Epoch valid loss 0.03577178039062673\n",
      "Validation Accuracy: 0.813301282051282\n",
      "Epoch 175\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.0056494073033881605\n",
      "Epoch valid loss 0.0355997443652879\n",
      "Validation Accuracy: 0.8152043269230769\n",
      "Epoch 176\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005456440906625233\n",
      "Epoch valid loss 0.03658365308999633\n",
      "Validation Accuracy: 0.8084935897435898\n",
      "Epoch 177\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005545179608115109\n",
      "Epoch valid loss 0.03554070978544843\n",
      "Validation Accuracy: 0.8134014423076926\n",
      "Epoch 178\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005449072356880951\n",
      "Epoch valid loss 0.03636672953143716\n",
      "Validation Accuracy: 0.8093950320512818\n",
      "Epoch 179\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005481955513206469\n",
      "Epoch valid loss 0.03624328523157881\n",
      "Validation Accuracy: 0.8110977564102563\n",
      "Epoch 180\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005408074426244189\n",
      "Epoch valid loss 0.036533858137539565\n",
      "Validation Accuracy: 0.8083934294871795\n",
      "Epoch 181\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005410492560147676\n",
      "Epoch valid loss 0.03708908530787971\n",
      "Validation Accuracy: 0.8056891025641024\n",
      "Epoch 182\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005422772259437314\n",
      "Epoch valid loss 0.03702750904724384\n",
      "Validation Accuracy: 0.8062900641025642\n",
      "Epoch 183\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005429657777420647\n",
      "Epoch valid loss 0.03634775831149174\n",
      "Validation Accuracy: 0.809294871794872\n",
      "Epoch 184\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005405061059037546\n",
      "Epoch valid loss 0.03601246019108938\n",
      "Validation Accuracy: 0.8113982371794871\n",
      "Epoch 185\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.00534499674952742\n",
      "Epoch valid loss 0.035654468497691244\n",
      "Validation Accuracy: 0.8131009615384616\n",
      "Epoch 186\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.00534557716095044\n",
      "Epoch valid loss 0.03562768534398996\n",
      "Validation Accuracy: 0.8134014423076925\n",
      "Epoch 187\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005307835251897149\n",
      "Epoch valid loss 0.03649318240320262\n",
      "Validation Accuracy: 0.8080929487179487\n",
      "Epoch 188\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005251742853812956\n",
      "Epoch valid loss 0.0363470958904005\n",
      "Validation Accuracy: 0.8098958333333333\n",
      "Epoch 189\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005381237408090402\n",
      "Epoch valid loss 0.036568121351779274\n",
      "Validation Accuracy: 0.8095953525641024\n",
      "Epoch 190\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005372135386817325\n",
      "Epoch valid loss 0.036150010786234185\n",
      "Validation Accuracy: 0.8102964743589742\n",
      "Epoch 191\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005360089985005331\n",
      "Epoch valid loss 0.036449219652403816\n",
      "Validation Accuracy: 0.8090945512820513\n",
      "Epoch 192\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005399884240964975\n",
      "Epoch valid loss 0.03595845567850539\n",
      "Validation Accuracy: 0.8114983974358976\n",
      "Epoch 193\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005373473414583436\n",
      "Epoch valid loss 0.03642703756952706\n",
      "Validation Accuracy: 0.809294871794872\n",
      "Epoch 194\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005381182958333105\n",
      "Epoch valid loss 0.03631836337109025\n",
      "Validation Accuracy: 0.8105969551282048\n",
      "Epoch 195\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005293014173384207\n",
      "Epoch valid loss 0.0365266693302263\n",
      "Validation Accuracy: 0.8088942307692307\n",
      "Epoch 196\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005249988353065173\n",
      "Epoch valid loss 0.03575651981055927\n",
      "Validation Accuracy: 0.8138020833333334\n",
      "Epoch 197\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.00533838108134739\n",
      "Epoch valid loss 0.03615868587094622\n",
      "Validation Accuracy: 0.8104967948717945\n",
      "Epoch 198\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.0053175195688364724\n",
      "Epoch valid loss 0.03624437450288007\n",
      "Validation Accuracy: 0.810997596153846\n",
      "Epoch 199\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005236656567091584\n",
      "Epoch valid loss 0.03563178996913708\n",
      "Validation Accuracy: 0.8135016025641022\n",
      "Epoch 200\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 \n",
      "Epoch training loss 0.005325421853415843\n",
      "Epoch valid loss 0.03655202206797325\n",
      "Validation Accuracy: 0.8087940705128205\n"
     ]
    }
   ],
   "source": [
    "losses, val_losses, accuracy = train(model, num_epochs, dl_train, dl_test, optimizer, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79c2899a-2c07-482d-9c21-ad0674462430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20dfb92adc0>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot([loss / len(dl_train) for loss in losses])\n",
    "plt.plot([val_loss / len(dl_test) for val_loss in val_losses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d26cabe-d76e-40c7-9d32-583deca78eea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
